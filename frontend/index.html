<!DOCTYPE HTML>
<html>
<head>
<title>research trends</title>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1, user-scalable=no" name="viewport"/>
<link href="assets/css/main.css" rel="stylesheet"/>
<link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet"/>
<link href="lib/lightbox/css/lightbox.min.css" rel="stylesheet"/>
<noscript><link href="assets/css/noscript.css" rel="stylesheet"/></noscript>
<script src="https://cdn.tailwindcss.com"></script>
<link crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" rel="stylesheet"/>
<script crossorigin="anonymous" defer="" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
<script crossorigin="anonymous" defer="" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/@alpinejs/collapse@3.x.x/dist/cdn.min.js"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
</head>
<body class="is-preload">
<header class="header">
<a class="logo" href="#">Research Trends</a>
</header>
<!-- Wrapper -->
<div class="divided" id="wrapper">
<!-- Five -->
<section class="wrapper style1 align-center">
<div class="inner">
<h2>Gallery</h2>
<p>Welcome to our curated gallery of groundbreaking research, exploring the latest in science, technology, and academia. Here, we showcase a selection of articles from the preprint server arXiv, a rich repository of scientific knowledge spanning a wide range of disciplines including machine learning and more.</p>
</div>
<!-- Gallery -->
<div class="gallery style2 medium lightbox onscroll-fade-in owl-carousel clients-carousel"><article><a class="image" href="images/fulls/01.jpg"><img alt="RAG" src="images/thumbs/01.jpg"/></a><div class="caption"><h3>RAG</h3><p>RAG (Retrieval-Augmented Generation) is a machine learning approach that combines information retrieval from external sources with generative models to improve the accuracy and relevance of responses.</p><ul class="actions fixed"><li><span class="button small">Details</span></li></ul></div></article><article><a class="image" href="images/fulls/02.jpg"><img alt="LLM" src="images/thumbs/02.jpg"/></a><div class="caption"><h3>LLM</h3><p>A Large Language Model (LLM) is an AI model trained on vast amounts of text data to understand, generate, and manipulate human language with high proficiency.</p><ul class="actions fixed"><li><span class="button small">Details</span></li></ul></div></article><article><a class="image" href="images/fulls/03.jpg"><img alt="Autonomous Driving" src="images/thumbs/03.jpg"/></a><div class="caption"><h3>Autonomous Driving</h3><p>Scene understanding in autonomous driving involves interpreting and analyzing the environment around a vehicle using sensors and AI to make real-time decisions for safe navigation.</p><ul class="actions fixed"><li><span class="button small">Details</span></li></ul></div></article><article><a class="image" href="images/fulls/04.jpg"><img alt="Data Mining" src="images/thumbs/04.jpg"/></a><div class="caption"><h3>Data Mining</h3><p>Data mining in automotive refers to the process of analyzing large datasets from vehicles and sensors to uncover patterns, trends, and insights that can improve performance, safety, and customer experience.</p><ul class="actions fixed"><li><span class="button small">Details</span></li></ul></div></article><article><a class="image" href="images/fulls/05.jpg"><img alt="CLIP" src="images/thumbs/05.jpg"/></a><div class="caption"><h3>CLIP</h3><p>CLIP (Contrastive Language-Image Pretraining) is a model that enables multimodal search by learning to connect images and text, allowing for more accurate and flexible search across visual and textual data.</p><ul class="actions fixed"><li><span class="button small">Details</span></li></ul></div></article></div>
</section>
<!-- Six -->
<section class="topics wrapper style1 align-center"><div class="inner"><h2>RAG</h2><div class="items style1 medium onscroll-fade-in"><section><div x-data="{open: false}"><span class="icon style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>The paper introduces EICopilot, an novel agent-based solution enhancing
search and exploration of enterprise registration data within extensive online
knowledge graphs like those detailing legal entities, registered capital, and
major shareholders. Traditional methods necessitate text-based queries and
manual subgraph explorations, often resulting in time-consuming processes.
EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this
landscape by utilizing Large Language Models (LLMs) to interpret natural
language queries. This solution automatically generates and executes Gremlin
scripts, providing efficient summaries of complex enterprise relationships.
Distinct feature a data pre-processing pipeline that compiles and annotates
representative queries into a vector database of examples for In-context
learning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought
with ICL to enhance Gremlin script generation for knowledge graph search and
exploration, and a novel query masking strategy that improves intent
recognition for heightened script accuracy. Empirical evaluations demonstrate
the superior performance of EICopilot, including speed and accuracy, over
baseline methods, with the \emph{Full Mask} variant achieving a syntax error
rate reduction to as low as 10.00% and an execution correctness of up to
82.14%. These components collectively contribute to superior querying
capabilities and summarization of intricate datasets, positioning EICopilot as
a groundbreaking tool in the exploration and exploitation of large-scale
knowledge graphs for enterprise information search.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13746v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Do as We Do, Not as You Think: the Conformity of Large Language Models</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Recent advancements in large language models (LLMs) revolutionize the field
of intelligent agents, enabling collaborative multi-agent systems capable of
tackling complex problems across various domains. However, the potential of
conformity within these systems, analogous to phenomena like conformity bias
and groupthink in human group dynamics, remains largely unexplored, raising
concerns about their collective problem-solving capabilities and possible
ethical implications. This paper presents a comprehensive study on conformity
in LLM-driven multi-agent systems, focusing on three aspects: the existence of
conformity, the factors influencing conformity, and potential mitigation
strategies. In particular, we introduce BenchForm, a new conformity-oriented
benchmark, featuring reasoning-intensive tasks and five distinct interaction
protocols designed to probe LLMs' behavior in collaborative scenarios. Several
representative LLMs are evaluated on BenchForm, using metrics such as
conformity rate and independence rate to quantify conformity's impact. Our
analysis delves into factors influencing conformity, including interaction time
and majority size, and examines how the subject agent rationalizes its
conforming behavior. Furthermore, we explore two strategies to mitigate
conformity effects, i.e., developing enhanced personas and implementing a
reflection mechanism. Several interesting findings regarding LLMs' conformity
are derived from empirical results and case studies. We hope that these
insights can pave the way for more robust and ethically-aligned collaborative
AI systems. Our benchmark and code are available at BenchForm.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13381v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Multi-agent systems must decide which agent is the most appropriate for a
given task. We propose a novel architecture for recommending which LLM agent
out of many should perform a task given a natural language prompt by extending
the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a
top-1 accuracy of 92.2% with each classification taking less than 300
milliseconds. In contrast to traditional classification methods, our
architecture is computationally cheap, adaptive to new classes, interpretable,
and controllable with arbitrary metrics through reinforcement learning. By
encoding natural language prompts into sentence embeddings, our model captures
the semantic content relevant to recommending an agent. The distance between
sentence embeddings that belong to the same agent is then minimized through
fine-tuning and aligned to human values through reinforcement learning from
human feedback. This allows the classification of natural language prompts
based on their nearest neighbors by measuring the cosine similarity between
embeddings. This work is made possible through the generation of a synthetic
dataset for agent recommendation, which we have open-sourced to the public
along with the code for AgentRec recommendation system at
https://github.com/joshprk/agentrec.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13333v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Materials discovery and design are essential for advancing technology across
various industries by enabling the development of application-specific
materials. Recent research has leveraged Large Language Models (LLMs) to
accelerate this process. We explore the potential of LLMs to generate viable
hypotheses that, once validated, can expedite materials discovery.
Collaborating with materials science experts, we curated a novel dataset from
recent journal publications, featuring real-world goals, constraints, and
methods for designing real-world applications. Using this dataset, we test
LLM-based agents that generate hypotheses for achieving given goals under
specific constraints. To assess the relevance and quality of these hypotheses,
we propose a novel scalable evaluation metric that emulates the process a
materials scientist would use to evaluate a hypothesis critically. Our curated
dataset, proposed method, and evaluation framework aim to advance future
research in accelerating materials discovery and design with LLMs.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13299v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">A RAG-Based Institutional Assistant</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Although large language models (LLMs) demonstrate strong text generation
capabilities, they struggle in scenarios requiring access to structured
knowledge bases or specific documents, limiting their effectiveness in
knowledge-intensive tasks. To address this limitation, retrieval-augmented
generation (RAG) models have been developed, enabling generative models to
incorporate relevant document fragments into their inputs. In this paper, we
design and evaluate a RAG-based virtual assistant specifically tailored for the
University of S\~ao Paulo. Our system architecture comprises two key modules: a
retriever and a generative model. We experiment with different types of models
for both components, adjusting hyperparameters such as chunk size and the
number of retrieved documents. Our optimal retriever model achieves a Top-5
accuracy of 30%, while our most effective generative model scores 22.04\%
against ground truth answers. Notably, when the correct document chunks are
supplied to the LLMs, accuracy significantly improves to 54.02%, an increase of
over 30 percentage points. Conversely, without contextual input, performance
declines to 13.68%. These findings highlight the critical role of database
access in enhancing LLM performance. They also reveal the limitations of
current semantic search methods in accurately identifying relevant documents
and underscore the ongoing challenges LLMs face in generating precise
responses.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13880v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing
external knowledge, its generation process heavily depends on the quality and
accuracy of the retrieved context. Large language models (LLMs) struggle to
evaluate the correctness of non-parametric knowledge retrieved externally when
it differs from internal memorization, leading to knowledge conflicts during
response generation. To this end, we introduce the Retrieval Preference
Optimization (RPO), a lightweight and effective alignment method to adaptively
leverage multi-source knowledge based on retrieval relevance. An implicit
representation of retrieval relevance is derived and incorporated into the
reward model to integrate retrieval evaluation and response generation into a
single model, solving the problem that previous methods necessitate the
additional procedure to assess the retrieval quality. Notably, RPO is the only
RAG-dedicated alignment approach that quantifies the awareness of retrieval
relevance in training, overcoming mathematical obstacles. Experiments on four
datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any
extra component, exhibiting its robust generalization.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13726v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Retrievals Can Be Detrimental: A Contrastive Backdoor Attack Paradigm on Retrieval-Augmented Diffusion Models</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Diffusion models (DMs) have recently demonstrated remarkable generation
capability. However, their training generally requires huge computational
resources and large-scale datasets. To solve these, recent studies empower DMs
with the advanced Retrieval-Augmented Generation (RAG) technique and propose
retrieval-augmented diffusion models (RDMs). By incorporating rich knowledge
from an auxiliary database, RAG enhances diffusion models' generation and
generalization ability while significantly reducing model parameters. Despite
the great success, RAG may introduce novel security issues that warrant further
investigation. In this paper, we reveal that the RDM is susceptible to backdoor
attacks by proposing a multimodal contrastive attack approach named BadRDM. Our
framework fully considers RAG's characteristics and is devised to manipulate
the retrieved items for given text triggers, thereby further controlling the
generated contents. Specifically, we first insert a tiny portion of images into
the retrieval database as target toxicity surrogates. Subsequently, a malicious
variant of contrastive learning is adopted to inject backdoors into the
retriever, which builds shortcuts from triggers to the toxicity surrogates.
Furthermore, we enhance the attacks through novel entropy-based selection and
generative augmentation strategies that can derive better toxicity surrogates.
Extensive experiments on two mainstream tasks demonstrate the proposed BadRDM
achieves outstanding attack effects while preserving the model's benign
utility.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13340v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Large Language Models (LLMs) have demonstrated powerful capabilities that
render them valuable in different applications, including conversational AI
products. It is paramount to ensure the security and reliability of these
products by mitigating their vulnerabilities towards malicious user
interactions, which can lead to the exposure of great risks and reputational
repercussions. In this work, we present a comprehensive study on the efficacy
of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs
that serve as input moderation guardrails. We systematically explore various
tuning methods by leveraging a small set of training data to adapt these models
as proxy defense mechanisms to detect malicious inputs and provide a reasoning
for their verdicts, thereby preventing the exploitation of conversational
agents. We rigorously evaluate the efficacy and robustness of different tuning
strategies to generalize across diverse adversarial and malicious query types.
Our experimental results outline the potential of alignment processes tailored
to a varied range of harmful input queries, even with constrained data
resources. These techniques significantly enhance the safety of conversational
AI systems and provide a feasible framework for deploying more secure and
trustworthy AI-driven interactions.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13080v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">FilmAgent: A Multi-Agent Framework for End-to-End Film Automation in Virtual 3D Spaces</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Virtual film production requires intricate decision-making processes,
including scriptwriting, virtual cinematography, and precise actor positioning
and actions. Motivated by recent advances in automated decision-making with
language agent-based societies, this paper introduces FilmAgent, a novel
LLM-based multi-agent collaborative framework for end-to-end film automation in
our constructed 3D virtual spaces. FilmAgent simulates various crew roles,
including directors, screenwriters, actors, and cinematographers, and covers
key stages of a film production workflow: (1) idea development transforms
brainstormed ideas into structured story outlines; (2) scriptwriting elaborates
on dialogue and character actions for each scene; (3) cinematography determines
the camera setups for each shot. A team of agents collaborates through
iterative feedback and revisions, thereby verifying intermediate scripts and
reducing hallucinations. We evaluate the generated videos on 15 ideas and 4 key
aspects. Human evaluation shows that FilmAgent outperforms all baselines across
all aspects and scores 3.98 out of 5 on average, showing the feasibility of
multi-agent collaboration in filmmaking. Further analysis reveals that
FilmAgent, despite using the less advanced GPT-4o model, surpasses the
single-agent o1, showing the advantage of a well-coordinated multi-agent
system. Lastly, we discuss the complementary strengths and weaknesses of
OpenAI's text-to-video model Sora and our FilmAgent in filmmaking.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.12909v1" target="_blank">full article</a></p></div></div></section></div></div><div class="inner"><h2>LLM</h2><div class="items style1 medium onscroll-fade-in"><section><div x-data="{open: false}"><span class="icon style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">EICopilot: Search and Explore Enterprise Information over Large-scale Knowledge Graphs with LLM-driven Agents</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>The paper introduces EICopilot, an novel agent-based solution enhancing
search and exploration of enterprise registration data within extensive online
knowledge graphs like those detailing legal entities, registered capital, and
major shareholders. Traditional methods necessitate text-based queries and
manual subgraph explorations, often resulting in time-consuming processes.
EICopilot, deployed as a chatbot via Baidu Enterprise Search, improves this
landscape by utilizing Large Language Models (LLMs) to interpret natural
language queries. This solution automatically generates and executes Gremlin
scripts, providing efficient summaries of complex enterprise relationships.
Distinct feature a data pre-processing pipeline that compiles and annotates
representative queries into a vector database of examples for In-context
learning (ICL), a comprehensive reasoning pipeline combining Chain-of-Thought
with ICL to enhance Gremlin script generation for knowledge graph search and
exploration, and a novel query masking strategy that improves intent
recognition for heightened script accuracy. Empirical evaluations demonstrate
the superior performance of EICopilot, including speed and accuracy, over
baseline methods, with the \emph{Full Mask} variant achieving a syntax error
rate reduction to as low as 10.00% and an execution correctness of up to
82.14%. These components collectively contribute to superior querying
capabilities and summarization of intricate datasets, positioning EICopilot as
a groundbreaking tool in the exploration and exploitation of large-scale
knowledge graphs for enterprise information search.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13746v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Do as We Do, Not as You Think: the Conformity of Large Language Models</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Recent advancements in large language models (LLMs) revolutionize the field
of intelligent agents, enabling collaborative multi-agent systems capable of
tackling complex problems across various domains. However, the potential of
conformity within these systems, analogous to phenomena like conformity bias
and groupthink in human group dynamics, remains largely unexplored, raising
concerns about their collective problem-solving capabilities and possible
ethical implications. This paper presents a comprehensive study on conformity
in LLM-driven multi-agent systems, focusing on three aspects: the existence of
conformity, the factors influencing conformity, and potential mitigation
strategies. In particular, we introduce BenchForm, a new conformity-oriented
benchmark, featuring reasoning-intensive tasks and five distinct interaction
protocols designed to probe LLMs' behavior in collaborative scenarios. Several
representative LLMs are evaluated on BenchForm, using metrics such as
conformity rate and independence rate to quantify conformity's impact. Our
analysis delves into factors influencing conformity, including interaction time
and majority size, and examines how the subject agent rationalizes its
conforming behavior. Furthermore, we explore two strategies to mitigate
conformity effects, i.e., developing enhanced personas and implementing a
reflection mechanism. Several interesting findings regarding LLMs' conformity
are derived from empirical results and case studies. We hope that these
insights can pave the way for more robust and ethically-aligned collaborative
AI systems. Our benchmark and code are available at BenchForm.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13381v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">AgentRec: Agent Recommendation Using Sentence Embeddings Aligned to Human Feedback</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Multi-agent systems must decide which agent is the most appropriate for a
given task. We propose a novel architecture for recommending which LLM agent
out of many should perform a task given a natural language prompt by extending
the Sentence-BERT (SBERT) encoder model. On test data, we are able to achieve a
top-1 accuracy of 92.2% with each classification taking less than 300
milliseconds. In contrast to traditional classification methods, our
architecture is computationally cheap, adaptive to new classes, interpretable,
and controllable with arbitrary metrics through reinforcement learning. By
encoding natural language prompts into sentence embeddings, our model captures
the semantic content relevant to recommending an agent. The distance between
sentence embeddings that belong to the same agent is then minimized through
fine-tuning and aligned to human values through reinforcement learning from
human feedback. This allows the classification of natural language prompts
based on their nearest neighbors by measuring the cosine similarity between
embeddings. This work is made possible through the generation of a synthetic
dataset for agent recommendation, which we have open-sourced to the public
along with the code for AgentRec recommendation system at
https://github.com/joshprk/agentrec.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13333v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">A RAG-Based Institutional Assistant</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Although large language models (LLMs) demonstrate strong text generation
capabilities, they struggle in scenarios requiring access to structured
knowledge bases or specific documents, limiting their effectiveness in
knowledge-intensive tasks. To address this limitation, retrieval-augmented
generation (RAG) models have been developed, enabling generative models to
incorporate relevant document fragments into their inputs. In this paper, we
design and evaluate a RAG-based virtual assistant specifically tailored for the
University of S\~ao Paulo. Our system architecture comprises two key modules: a
retriever and a generative model. We experiment with different types of models
for both components, adjusting hyperparameters such as chunk size and the
number of retrieved documents. Our optimal retriever model achieves a Top-5
accuracy of 30%, while our most effective generative model scores 22.04\%
against ground truth answers. Notably, when the correct document chunks are
supplied to the LLMs, accuracy significantly improves to 54.02%, an increase of
over 30 percentage points. Conversely, without contextual input, performance
declines to 13.68%. These findings highlight the critical role of database
access in enhancing LLM performance. They also reveal the limitations of
current semantic search methods in accurately identifying relevant documents
and underscore the ongoing challenges LLMs face in generating precise
responses.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13880v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">RPO: Retrieval Preference Optimization for Robust Retrieval-Augmented Generation</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>While Retrieval-Augmented Generation (RAG) has exhibited promise in utilizing
external knowledge, its generation process heavily depends on the quality and
accuracy of the retrieved context. Large language models (LLMs) struggle to
evaluate the correctness of non-parametric knowledge retrieved externally when
it differs from internal memorization, leading to knowledge conflicts during
response generation. To this end, we introduce the Retrieval Preference
Optimization (RPO), a lightweight and effective alignment method to adaptively
leverage multi-source knowledge based on retrieval relevance. An implicit
representation of retrieval relevance is derived and incorporated into the
reward model to integrate retrieval evaluation and response generation into a
single model, solving the problem that previous methods necessitate the
additional procedure to assess the retrieval quality. Notably, RPO is the only
RAG-dedicated alignment approach that quantifies the awareness of retrieval
relevance in training, overcoming mathematical obstacles. Experiments on four
datasets demonstrate that RPO outperforms RAG by 4-10% in accuracy without any
extra component, exhibiting its robust generalization.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13726v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Refining Input Guardrails: Enhancing LLM-as-a-Judge Efficiency Through Chain-of-Thought Fine-Tuning and Alignment</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Large Language Models (LLMs) have demonstrated powerful capabilities that
render them valuable in different applications, including conversational AI
products. It is paramount to ensure the security and reliability of these
products by mitigating their vulnerabilities towards malicious user
interactions, which can lead to the exposure of great risks and reputational
repercussions. In this work, we present a comprehensive study on the efficacy
of fine-tuning and aligning Chain-of-Thought (CoT) responses of different LLMs
that serve as input moderation guardrails. We systematically explore various
tuning methods by leveraging a small set of training data to adapt these models
as proxy defense mechanisms to detect malicious inputs and provide a reasoning
for their verdicts, thereby preventing the exploitation of conversational
agents. We rigorously evaluate the efficacy and robustness of different tuning
strategies to generalize across diverse adversarial and malicious query types.
Our experimental results outline the potential of alignment processes tailored
to a varied range of harmful input queries, even with constrained data
resources. These techniques significantly enhance the safety of conversational
AI systems and provide a feasible framework for deploying more secure and
trustworthy AI-driven interactions.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13080v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">ACEBench: Who Wins the Match Point in Tool Learning?</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Large language models (LLMs) have demonstrated significant potential in
decision-making and reasoning, especially when combined with various tools to
effectively solve complex problems. However, existing evaluation systems for
assessing LLM function calling capabilities have several limitations: (1)
limited evaluation scenarios, lacking assessments in real multi-turn dialogue
contexts; (2) narrow evaluation dimensions, lacking detailed assessments for
fine-grained function calls; (3) relying on LLMs or real API executions for
result evaluation, which introduces significant overhead. To address these
issues, we propose a comprehensive evaluation system named ACEBench. This
system is meticulously designed to encompass a wide spectrum of function
calling scenarios. Moreover, it categorizes these scenarios into three primary
types according to the evaluation methodology: Normal, Special, and Agent.
Normal evaluates function calls in basic scenarios; Special evaluates function
calls in scenarios with vague or incomplete instructions; Agent introduces
multi-agent interactions to simulate function calling evaluation in real-world
multi-turn interactions. We conducted extensive experiments on ACEBench,
analyzing various LLMs in-depth and performing a more granular analysis of
error causes across different data types.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.12851v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">RAG-Reward: Optimizing RAG with Reward Modeling and RLHF</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs)
with relevant and up-to-date knowledge, improving their ability to answer
knowledge-intensive questions. It has been shown to enhance both generation
quality and trustworthiness. While numerous works have focused on improving
retrieval, generation, and evaluation, the role of reward models in
reinforcement learning for optimizing RAG and establishing automated
benchmarking pipelines remains underexplored. In this paper, we introduce
\textbf{RAG-Reward}, a dataset designed to enable \textit{hallucination-free,
comprehensive, reliable, and efficient RAG}. We define four key metrics for
assessing generation quality and develop an automated annotation pipeline that
leverages multiple LLMs to generate outputs across diverse RAG scenarios.
GPT-4o is used to evaluate and construct preference data. Using
\textbf{RAG-Reward}, we train reward models and apply reinforcement learning
with human feedback (RLHF) to improve LLMs' effectiveness in RAG. Experimental
results show that our reward model achieves state-of-the-art performance on a
held-out test set, demonstrating both the effectiveness of our approach and the
quality of our dataset. Furthermore, the improved generation quality of the
trained policy model highlights the feasibility of using RLHF to enhance RAG
pipelines.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.13264v1" target="_blank">full article</a></p></div></div></section><section><div x-data="{open: false}"><span class="icon solid style2 major fa-gem"></span><h3 @click="open = ! open" class="hover:underline cursor-pointer decoration-2 decoration-green-600 text-gray-800 text-sm">Adaptive Retrieval Without Self-Knowledge? Bringing Uncertainty Back Home</h3><div class="text-sm text-gray-500 pt-2" x-collapse.duration.500ms="" x-show="open"><p>Retrieval Augmented Generation (RAG) improves correctness of Question
Answering (QA) and addresses hallucinations in Large Language Models (LLMs),
yet greatly increase computational costs. Besides, RAG is not always needed as
may introduce irrelevant information. Recent adaptive retrieval methods
integrate LLMs' intrinsic knowledge with external information appealing to LLM
self-knowledge, but they often neglect efficiency evaluations and comparisons
with uncertainty estimation techniques. We bridge this gap by conducting a
comprehensive analysis of 35 adaptive retrieval methods, including 8 recent
approaches and 27 uncertainty estimation techniques, across 6 datasets using 10
metrics for QA performance, self-knowledge, and efficiency. Our findings show
that uncertainty estimation techniques often outperform complex pipelines in
terms of efficiency and self-knowledge, while maintaining comparable QA
performance.</p><p class="pb-2 pt-2 text-center"><a class="underline decoration-2 text-green-600 text-md pt-2" href="http://arxiv.org/abs/2501.12835v1" target="_blank">full article</a></p></div></div></section></div></div></section>
<!-- Footer -->
<footer class="wrapper style1 align-center">
<div class="inner">
<ul class="icons">
<li><a class="icon brands style2 fa-twitter" href="#"><span class="label">Twitter</span></a></li>
<li><a class="icon brands style2 fa-facebook-f" href="#"><span class="label">Facebook</span></a></li>
<li><a class="icon brands style2 fa-instagram" href="#"><span class="label">Instagram</span></a></li>
<li><a class="icon brands style2 fa-linkedin-in" href="#"><span class="label">LinkedIn</span></a></li>
<li><a class="icon style2 fa-envelope" href="#"><span class="label">Email</span></a></li>
</ul>
<p>Â© Copyrights: <a href="https://aibricks.tech">aibricks.tech</a>.</p>
</div>
</footer>
<!-- Chatbot -->
<div id="chatbot">
<div id="chatbot-header">Chatbot</div>
<div id="chatbot-body">
<p class="agent-message">Agent: Hi! How can I help you today?</p>
</div>
<div id="chatbot-input">
<input id="chatbot-message" onkeydown="if(event.key === 'Enter') sendMessage()" placeholder="Type a message..." type="text"/>
<button onclick="sendMessage()">Send</button>
</div>
</div>
<button id="chatbot-toggle" onclick="toggleChatbot()">ðŸ’¬</button>
</div>
<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrollex.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>
<script src="lib/owlcarousel/owl.carousel.min.js"></script>
<script src="lib/lightbox/js/lightbox.min.js"></script>
<script>
				function toggleChatbot() {
					var chatbot = document.getElementById('chatbot');
					if (chatbot.style.display === 'none' || chatbot.style.display === '') {
						chatbot.style.display = 'flex';
					} else {
						chatbot.style.display = 'none';
					}
				}
		
				async function sendMessage() {
					var messageInput = document.getElementById('chatbot-message');
					var message = messageInput.value;
					if (message.trim() !== '') {
						var chatbotBody = document.getElementById('chatbot-body');
						var userMessage = document.createElement('p');
						userMessage.textContent = "User: "+message;
						userMessage.className = 'user-message';
						chatbotBody.appendChild(userMessage);
						messageInput.value = '';
						chatbotBody.scrollTop = chatbotBody.scrollHeight;

						// Simulate AI response
						var aiResponse = document.createElement('p');
						aiResponse.textContent = 'Agent: AI is typing...';
						aiResponse.className = 'agent-message';
						chatbotBody.appendChild(aiResponse);
						chatbotBody.scrollTop = chatbotBody.scrollHeight;

						try {
							const response = await fetch('https://reportbot-hmg0c6gzccdgcsb8.germanywestcentral-01.azurewebsites.net/api/chat', {
								method: 'POST',
								headers: {
									'Content-Type': 'application/json',
									'Access-Control-Allow-Origin': 'https://reportbot-hmg0c6gzccdgcsb8.germanywestcentral-01.azurewebsites.net'
								},
								body: JSON.stringify({ message: message })
							});
							// Simulate a fetch call with a dummy response
							// const response = await new Promise((resolve) => {
							// 	setTimeout(() => {
							// 		resolve({
							// 			ok: true,
							// 			json: async () => ({ message: 'This is a dummy response from AI. [feature intergrating db with azure openai is coming soon.]' }),
							// 			body: {
							// 				getReader: () => {
							// 					const text = 'This is a dummy response from AI. [feature intergrating db with azure openai is coming soon.]';
							// 					let index = 0;
							// 					return {
							// 						read: async () => {
							// 							if (index < text.length) {
							// 								const value = new TextEncoder().encode(text.slice(index, index + 1));
							// 								index++;
							// 								return { done: false, value };
							// 							} else {
							// 								return { done: true, value: new Uint8Array() };
							// 							}
							// 						}
							// 					};
							// 				}
							// 			}
							// 		});
							// 	}, 1000);
							// });

							if (response.ok) {
								const data = await response.json();
								aiResponse.textContent = "Agent: " + data.answer;
							} else {
								aiResponse.textContent = 'Error: Unable to get AI response.';
							}
						} catch (error) {
							aiResponse.textContent = 'Error: ' + error.message;
						}

						chatbotBody.scrollTop = chatbotBody.scrollHeight;
					}
				}
			</script>
</body>
</html>